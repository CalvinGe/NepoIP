
CondaError: Run 'conda init' before 'conda activate'

[W116 22:05:11.510665514 init.cpp:855] Warning: Use _jit_set_fusion_strategy, bailout depth is deprecated. Setting to (STATIC, 2) (function operator())
Torch device: cpu
Processing dataset...
Loaded data: Batch(batch=[110000], cell=[5000, 3, 3], edge_cell_shift=[1259852, 3], edge_index=[2, 1259852], elec_potential=[110000, 1], forces=[110000, 3], grad_factor=[110000, 22, 3], pbc=[5000, 3], pos=[110000, 3], ptr=[5001], total_energy=[5000, 1])
    processed data size: ~65.36 MB
Cached processed data to disk
Done!
Successfully loaded the data set of type NpzDataset(5000)...
Replace string dataset_forces_rms to 15.625872611999512
Replace string dataset_per_atom_total_energy_mean to -104.26958465576172
Atomic outputs are scaled by: [H, C, N, O: 15.625873], shifted by [H, C, N, O: -104.269585].
Replace string dataset_forces_rms to 15.625872611999512
Initially outputs are globally scaled by: 15.625872611999512, total_energy are globally shifted by None.
Successfully built the network...
Number of weights: 425624
Number of trainable weights: 425624
! Starting training ...

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse       e_rmse     e/N_rmse
      0    10        0.998        0.998     0.000205         10.1         15.6         4.92        0.224
      0    20        0.922        0.922     0.000256         10.3           15          5.5         0.25
      0    30        0.842        0.842     0.000261         9.63         14.3         5.56        0.253
      0    40         1.13         1.13     0.000257         10.7         16.6         5.51         0.25
      0    50         1.16         1.16     0.000419         10.7         16.8         7.04         0.32
      0    60        0.938        0.938     0.000257         10.1         15.1         5.51         0.25
      0    70        0.994        0.994      0.00036         10.1         15.6         6.52        0.296
      0    80            1            1     0.000277         10.5         15.7         5.72         0.26
      0    90         1.08         1.08     0.000292         10.9         16.2         5.87        0.267
      0   100         1.01         1.01     0.000307         10.2         15.7         6.02        0.274


  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse       e_rmse     e/N_rmse
! Initial Validation          0   98.096    0.005        0.997      0.00029        0.997         10.3         15.6         5.86        0.266
Wall time: 98.09695302322507
! Best model        0    0.997

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse       e_rmse     e/N_rmse
      1    10         0.71        0.707      0.00372         8.09         13.1           21        0.953
      1    20        0.463        0.449       0.0141         6.89         10.5         40.8         1.85
      1    30        0.253        0.249      0.00338         5.23          7.8           20        0.909
      1    40        0.182        0.182     0.000379         4.42         6.66         6.69        0.304
      1    50         0.17        0.169     0.000251         4.22         6.43         5.45        0.248
      1    60        0.209        0.209     0.000222         4.76         7.14         5.12        0.233
      1    70        0.174        0.173      0.00103          4.4          6.5           11          0.5
      1    80        0.152        0.152      0.00023          4.1         6.09         5.21        0.237
      1    90        0.238        0.237     0.000206         4.58         7.61         4.93        0.224
      1   100         0.12         0.12     7.85e-05         3.63          5.4         3.05        0.138
      1   110        0.102        0.102     0.000169         3.45         4.99         4.46        0.203
      1   120        0.121        0.121     0.000101         3.68         5.43         3.45        0.157
      1   130        0.128        0.127     0.000647         3.51         5.57         8.75        0.398
      1   140        0.139        0.139     0.000484         3.94         5.82         7.56        0.344
      1   150         0.12         0.12     9.86e-05         3.56         5.41         3.41        0.155
      1   160        0.104        0.104     0.000111         3.45         5.04         3.62        0.165
      1   170          0.1          0.1     0.000135         3.18         4.95            4        0.182
      1   180       0.0789       0.0789     1.47e-05         3.01         4.39         1.32         0.06
      1   190       0.0645       0.0641     0.000411         2.65         3.96         6.97        0.317
      1   200        0.103        0.103     4.19e-05         3.18         5.03         2.23        0.101
      1   210       0.0671       0.0669     0.000192         2.71         4.04         4.77        0.217
      1   220       0.0612        0.061     0.000198         2.72         3.86         4.83         0.22
      1   230       0.0729       0.0729     5.56e-05         2.86         4.22         2.56        0.117
      1   240       0.0478       0.0473     0.000528         2.43          3.4          7.9        0.359
      1   250       0.0737       0.0736     6.32e-05          2.7         4.24         2.73        0.124
      1   260       0.0542        0.054      0.00021         2.55         3.63         4.98        0.226
      1   270       0.0463       0.0463     1.56e-05         2.41         3.36         1.36       0.0618
      1   280       0.0386       0.0386     2.18e-05         2.18         3.07          1.6       0.0729
      1   290       0.0395       0.0395     3.19e-05          2.1          3.1         1.94       0.0882
      1   300       0.0682       0.0679     0.000246         2.81         4.07         5.39        0.245
      1   310       0.0608       0.0607     0.000145         2.52         3.85         4.14        0.188
      1   320       0.0391        0.039      0.00014         2.25         3.08         4.07        0.185
      1   330       0.0484       0.0484     1.89e-05         2.34         3.44          1.5        0.068
      1   340       0.0531        0.053     0.000128          2.4          3.6         3.89        0.177
      1   350       0.0345       0.0344     8.84e-05         1.97          2.9         3.23        0.147
      1   360       0.0325       0.0325     1.03e-05         2.12         2.82          1.1       0.0502
      1   370       0.0673       0.0673     5.86e-05         2.59         4.05         2.63         0.12
      1   380       0.0302       0.0302     1.55e-05          1.8         2.72         1.35       0.0616
      1   390       0.0666       0.0665       0.0001         2.49         4.03         3.44        0.156
      1   400       0.0352       0.0351      0.00014         1.91         2.93         4.06        0.185
      1   410        0.031       0.0309      9.5e-05          1.9         2.74         3.35        0.152
      1   420       0.0418       0.0417     8.04e-05         2.19         3.19         3.08         0.14
      1   430       0.0316       0.0315     6.32e-05         1.86         2.77         2.73        0.124
      1   440       0.0325       0.0324     8.92e-05         1.88         2.81         3.25        0.148
      1   450       0.0399       0.0399     6.38e-06         2.17         3.12        0.868       0.0395
      1   460       0.0543       0.0543     3.21e-05         2.38         3.64         1.95       0.0885
      1   470       0.0262       0.0262     2.35e-05         1.83         2.53         1.67       0.0758
      1   480       0.0314       0.0313     0.000134         2.06         2.76         3.98        0.181
      1   490       0.0274       0.0274     3.32e-05         1.78         2.59         1.98         0.09
      1   500       0.0413       0.0412     0.000119         2.18         3.17         3.74         0.17
      1   510       0.0387       0.0386     0.000144         2.05         3.07         4.12        0.187
      1   520       0.0227       0.0226     9.32e-05         1.73         2.35         3.32        0.151
      1   530       0.0192        0.019     0.000249         1.53         2.15         5.42        0.247
      1   540       0.0282       0.0282     1.52e-05         1.83         2.62         1.34       0.0608
      1   550       0.0388       0.0387      2.3e-05         1.99         3.08         1.65       0.0749
      1   560       0.0231        0.023     9.23e-05         1.72         2.37          3.3         0.15
      1   570       0.0228       0.0228      3.9e-05         1.66         2.36         2.15       0.0976
      1   580       0.0246       0.0245     6.69e-05          1.7         2.45         2.81        0.128
      1   590       0.0235       0.0235      1.5e-05         1.57          2.4         1.33       0.0605
      1   600       0.0218       0.0216     0.000136         1.51          2.3         4.01        0.182
      1   610       0.0244       0.0243     5.43e-05         1.63         2.44         2.53        0.115
      1   620       0.0284       0.0284      3.2e-05         1.85         2.63         1.95       0.0885
      1   630       0.0243       0.0243     6.91e-05          1.7         2.43         2.86         0.13
      1   640       0.0242       0.0242     7.12e-06         1.58         2.43        0.917       0.0417
      1   650       0.0215       0.0214     9.52e-05         1.61         2.28         3.36        0.153
      1   660       0.0193       0.0192     0.000106         1.54         2.16         3.54        0.161
      1   670       0.0156       0.0156     2.42e-05         1.44         1.95         1.69       0.0769
      1   680        0.016       0.0159     0.000108         1.42         1.97         3.58        0.163
      1   690       0.0278       0.0277     4.95e-05         1.81          2.6         2.42         0.11
      1   700       0.0309       0.0308     0.000122          1.9         2.74          3.8        0.173
      1   710       0.0199       0.0199     1.69e-05         1.54         2.21         1.41       0.0643
      1   720       0.0333       0.0332     0.000109         1.78         2.85         3.58        0.163
      1   730        0.019       0.0189     8.07e-05         1.49         2.15         3.09         0.14
      1   740       0.0134       0.0134     4.09e-05          1.3         1.81          2.2          0.1
      1   750       0.0227       0.0226     1.61e-05         1.61         2.35         1.38       0.0627
      1   760       0.0237       0.0236     6.16e-05         1.67          2.4          2.7        0.123
      1   770       0.0239       0.0239        1e-05         1.65         2.42         1.09       0.0494
      1   780       0.0237       0.0236     2.62e-05         1.65          2.4         1.76         0.08
      1   790       0.0238       0.0238     2.97e-05          1.7         2.41         1.87       0.0852
      1   800        0.019        0.019     4.09e-05         1.48         2.15          2.2       0.0999

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse       e_rmse     e/N_rmse
      1    10       0.0148       0.0148     2.21e-05         1.35          1.9         1.62       0.0734
      1    20       0.0166       0.0166     2.21e-05         1.38         2.01         1.62       0.0735
      1    30       0.0147       0.0147      1.9e-05         1.31         1.89          1.5       0.0681
      1    40       0.0159       0.0159     2.45e-05         1.38         1.97          1.7       0.0773
      1    50       0.0134       0.0134     9.72e-06         1.28         1.81         1.07       0.0487
      1    60       0.0245       0.0245     2.91e-05         1.63         2.45         1.85       0.0842
      1    70       0.0165       0.0165     2.34e-05         1.37            2         1.66       0.0756
      1    80       0.0193       0.0193     1.58e-05         1.49         2.17         1.37       0.0621
      1    90       0.0187       0.0187      2.8e-05         1.47         2.14         1.82       0.0827
      1   100       0.0166       0.0166     1.01e-05         1.36         2.01         1.09       0.0496


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse       e_rmse     e/N_rmse
! Train               1 1491.795    0.005       0.0827     0.000467       0.0831         2.62         4.49         7.43        0.338
! Validation          1 1491.795    0.005       0.0179     1.85e-05       0.0179         1.41         2.09         1.48       0.0671
Wall time: 1491.7952804900706
! Best model        1    0.018

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse       e_rmse     e/N_rmse
      2    10        0.025       0.0249     1.95e-05         1.62         2.47         1.52        0.069
      2    20       0.0234       0.0234     6.01e-06          1.6         2.39        0.843       0.0383
slurmstepd: error: *** JOB 1722072 ON node081 CANCELLED AT 2025-01-16T22:30:55 ***
